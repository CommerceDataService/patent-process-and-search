{"type": "oa", "appid": "13000099", "ifwnumber": "I0XTDP9BPXXIFW4", "documentcode": "CTFR", "documentsourceidentifier": "CTFR", "partyidentifier": "88343", "groupartunitnumber": "2487", "textdata": "The present application is being examined under the pre-AIA first to invent provisions. \n\nDETAILED ACTION\n\nResponse to Arguments\n\nApplicant amended claims 13, 16, and 17.\n\nApplicant amended claims 16 and 17 to overcome Examiner Objections.\n\nApplicant amended claim 13 beyond formalities and 35 USC 112 Rejections.\n\nApplicant amended the Specification to overcome Examiner Objections.  Examiner thanks the Applicant for their cooperation [Page 5 lines 2 \u2013 4].\n\nApplicant amended claim 13 to overcome 35 USC 101 Rejections [Page 5 lines 4 \u2013 6].\n\nApplicant amended claim 13 to overcome 35 USC 112 Enablement Rejections.  The Examiner maintains an indefinite rejection regarding the claimed term \u201cvalid\u201d.  Although the Applicant struck the term valid from \u201cvalid image patch\u201d [line 10], the term remains with regards to the \u201cvalid projection\u201d which results in the claimed \u201cvalid image patch\u201d [line 11].  Thus, the Indefinite Rejection is maintained.\n\nApplicant\u2019s arguments with respect to claim 13 have been considered but are moot because the arguments do not apply to any of the new references being used in the current rejection.\n\nApplicant requested reconsideration of the 35 USC 103(a) in view of the amended claim [Page 5 lines 7 \u2013 19].\n\nFirst, Applicant stated deficiencies of the references in general [Page 5 lines 7 \u2013 9].  However, the Examiner notes the features are not necessarily claimed.\n\nIn response to applicant's argument that the references fail to show certain features of applicant\u2019s invention, it is noted that the features upon which applicant relies (i.e., \u201cdifferent positions\u201d and \u201cdifferent lighting\u201d) are not recited in the rejected claim(s).  Although the claims are interpreted in light of the specification, limitations from the specification are not read into the claims.  See In re Van Geuns, 988 F.2d 1181, 26 USPQ2d 1057 (Fed. Cir. 1993).\n\nSecond, Applicant reiterated features of the claim the reference don\u2019t teach and alleges that the now defined \u201ctexel\u201d is not obvious to one of ordinary skill in the art [Page 5 lines 10 \u2013 14].  The Examiner considers Lepetit teaching \"texture maps\" as analogous to the claimed \u201ctexel grid\u201d [Lepetit Section 4.3].  The Examiner in view of the amended claim shall cite an additional reference to advance prosecution.\n\nThird, the Applicant alleges the references do not teach the features of the claim and in particular cites no teaching of a \u201cvalid projection\u201d [Page 5 lines 15 \u2013 19].  The Examiner notes Lepetit Section 4.4.2 \"interest point matching\u201d and Gu in Paragraphs 68 and 78 \u2013 82 teaches image patches and regions considered for matching / correction / and tests for validity of the region for correction.  The Examiner in view of the amended claim shall cite an additional reference to advance prosecution.\n\n\n\nElection/Restrictions\n\nApplicant canceled claims 1 \u2013 12.\n\nApplicant\u2019s election of Group II (claims 13 - 20) in the reply filed on September 3rd, 2013 is acknowledged.\n\nBecause applicant did not distinctly and specifically point out the supposed errors in the restriction requirement, the election has been treated as an election without traverse (MPEP \u00a7 818.03(a)).\n\n\n\nInformation Disclosure Statement\n\nThe information disclosure statement (IDS) submitted on June 23rd, 2011 was filed before the mailing date of the First Action on Merits mailed September 26th, 2013.  The submission is in compliance with the provisions of 37 CFR 1.97.  Accordingly, the information disclosure statement is being considered by the Examiner.\n\nThe listing of references in the Specification is not a proper information disclosure statement [Page 2 lines 29 \u2013 32].  37 CFR 1.98(b) requires a list of all patents, publications, or other information submitted for consideration by the Office, and MPEP \u00a7 609.04(a) states, \"the list may not be incorporated into the specification but must be submitted in a separate paper.\"  Therefore, unless the references have been cited by the Examiner on form PTO-892, they have not been considered.\n\n\n\nClaim Interpretation - 35 USC \u00a7 112 Functional Analysis\n\nThe following is a quotation of 35 U.S.C. 112(f):\n\n(f) Element in Claim for a Combination. \u2013 An element in a claim for a combination may be expressed as a means or step for performing a specified function without the recital of structure, material, or acts in support thereof, and such claim shall be construed to cover the corresponding structure, material, or acts described in the specification and equivalents thereof.\n\n\n\nThe following is a quotation of pre-AIA 35 U.S.C. 112, sixth paragraph:\n\nAn element in a claim for a combination may be expressed as a means or step for performing a specified function without the recital of structure, material, or acts in support thereof, and such claim shall be construed to cover the corresponding structure, material, or acts described in the specification and equivalents thereof.\n\n\n\nUse of the word \u201cmeans\u201d (or \u201cstep for\u201d) in a claim with functional language creates a rebuttable presumption that the claim element is to be treated in accordance with 35 U.S.C. 112(f) (pre-AIA 35 U.S.C. 112, sixth paragraph).  The presumption that 35 U.S.C. 112(f) (pre-AIA 35 U.S.C. 112, sixth paragraph) is invoked is rebutted when the function is recited with sufficient structure, material, or acts within the claim itself to entirely perform the recited function.  \n\nAbsence of the word \u201cmeans\u201d (or \u201cstep for\u201d) in a claim creates a rebuttable presumption that the claim element is not to be treated in accordance with 35 U.S.C. 112(f) (pre-AIA 35 U.S.C. 112, sixth paragraph).  The presumption that 35 U.S.C. 112(f) (pre-AIA 35 U.S.C. 112, sixth paragraph) is not invoked is rebutted when the claim element recites function but fails to recite sufficiently definite structure, material or acts to perform that function. \n\nClaim elements in this application that use the word \u201cmeans\u201d (or \u201cstep for\u201d) are presumed to invoke 35 U.S.C. 112(f) except as otherwise indicated in an Office action.  Similarly, claim elements that do not use the word \u201cmeans\u201d (or \u201cstep for\u201d) are presumed not to invoke 35 U.S.C. 112(f) except as otherwise indicated in an Office action.\n\nThe Examiner notes the phrases \u201ccontroller to compare\u201d and \u201ccontroller to perform\u201d may invoke functional analysis, however, when the \u201ccontroller\u201d claimed is considered and is coupled to \"a camera\u201d and \u201can inertial navigation system\u201d, the Examiner notes the \u201ccontroller\u201d is provided structure by being coupled to elements with defined structure.  Thus, functional analysis is not invoked of the claims.\n\n\n\nClaim Rejections - 35 USC \u00a7 112\n\nThe following is a quotation of 35 U.S.C. 112(b):\n\n(b)  CONCLUSION.\u2014The specification shall conclude with one or more claims particularly pointing out and distinctly claiming the subject matter which the inventor or a joint inventor regards as the invention.\n\n\n\n\n\nThe following is a quotation of 35 U.S.C. 112 (pre-AIA), second paragraph:\n\nThe specification shall conclude with one or more claims particularly pointing out and distinctly claiming the subject matter which the applicant regards as his invention.\n\n\n\n\n\nClaim 13 is rejected under 35 U.S.C. 112(b) or 35 U.S.C. 112 (pre-AIA), second paragraph, as being indefinite for failing to particularly point out and distinctly claim the subject matter which the inventor or a joint inventor, or for pre-AIA the applicant regards as the invention.\n\nThe term \"valid\" in claim 13 is a relative term which renders the claim indefinite.  The term \"valid\" is not defined by the claim, the specification does not provide a standard for ascertaining the requisite degree, and one of ordinary skill in the art would not be reasonably apprised of the scope of the invention.  The Examiner notes Specification Paragraph 20 mentions a \u201cvalid image patch\u201d, but does not provide criteria to determine if usage of such an image patch is valid.\n\nThe term \"imprecise\" in claim 13 is a relative term which renders the claim indefinite.  The term \"imprecise\" is not defined by the claim, the specification does not provide a standard for ascertaining the requisite degree, and one of ordinary skill in the art would not be reasonably apprised of the scope of the invention.\n\n\n\nClaim Rejections - 35 USC \u00a7 103\n\nThe following is a quotation of pre-AIA 35 U.S.C. 103(a) which forms the basis for all obviousness rejections set forth in this Office action:\n\n(a) A patent may not be obtained though the invention is not identically disclosed or described as set forth in section 102 of this title, if the differences between the subject matter sought to be patented and the prior art are such that the subject matter as a whole would have been obvious at the time the invention was made to a person having ordinary skill in the art to which said subject matter pertains.  Patentability shall not be negatived by the manner in which the invention was made.\n\n\n\n\n\nClaim 13 is rejected under pre-AIA 35 U.S.C. 103(a) as being unpatentable over Kishikawa (US PG PUB 2005/0177350 A1 referred to as \u201cKishikawa\u201d throughout), and further in view of Wang, et al. (Wang, P, Wang, T., Dayong, D., Zhang, Y., Bi, W., Bao, Y., \u201cMirror World Navigation for Mobile Users Based on Augmented Reality\u201d Proceedings of the 17th ACM International Conference on Multimedia, MM\u201909, October 19 \u2013 24, 2009, pg. 1025 \u2013 1026 referred to as \u201cWang\u201d throughout) [Cited in September 26th, 2013 Office Action], Lepetit, et al. (Lepetit, V., Fua, P.; \u201cMonocular Model-Based 3D Tracking of Rigid Objects: A Survey\u201d Foundations and Trends in Computer Graphics and Vision.  Vol. 1, No. 1, 2005, pp 1 \u2013 89 referred to as \u201cLepetit\u201d throughout) [Cited in Previous Actions], Gu (US PG PUB 2006/0210146 A1 referred to as \"Gu\" throughout), and Nehab, et al. (US PG PUB 2008/0309676 A1 referred to as \u201cNehab\u201d throughout).\n\n\tRegarding claim 13, Kishikawa teaches a device to aid in creation of a virtual world while Wang teaches a device with an inertial navigation system (INS) and sensor fusion techniques to process the data from the device to align elements photographed with the virtual / mirror world.  Lepetit teaches a variety of 3D image processing techniques including camera considerations, use of image patches, and edge matching.  Gu teaches further 3D image processing considerations including triangles / meshes, edge alignment, and corrections.  Nehab teaches usage of a texel matrix / grid for rendering 3D images.\n\n\tIt would have been obvious to one of ordinary skill in the art at the time the invention was made to combine Kishikawa's camera and imaging device to generate a virtual world with Wang\u2019s, Lepetit\u2019s, Gu\u2019s, and Nehab\u2019s image processing techniques processing techniques.  The combination teaches a hardware controller [Kishikawa Figure 5 reference character 100 (system) which the Examiner notes performs control functions.  The Examiner further considers a computer as a controller and thus Figure 9 reference character 27 (mobile computer) is also a controller (Kishikawa Paragraphs 91, 92, and 95).  Further Scott Figures 1 and 2 have elements of a control algorithm that could be considered as a control (e.g. fusion, rendering, estimating position, etc.).]; a camera coupled to said controller [Kishikawa Figure 9 reference character 20 (camera coupled to controller \u2013 Figure 9 reference character 27 \u2013 see remarks directed towards the claimed \u201ccontroller\u201d above.  Scott Paragraphs 56 and 59 teach the camera coupled to algorithms to compute position which is considered as \u201ccontrol\u201d to the Examiner.  Scott Figures 1 and 2 (see above)]; an inertial navigation system sensor coupled to said controller [Kishikawa Paragraphs 92 and 93 suggest an INS (GPS unit synchronized with the camera) and Paragraph 17 (description of functions in which one of ordinary skill would select an INS (inertial navigation system) to perform the functions.  Further Wang Section 2 Paragraphs 1 and 2 teach using an INS.]; wherein said controller to compare an input three dimensional geometric model contour with an edge feature of images from said camera [Wang Section 1 as well as Lepetit Section 4.1 (\u201cextract image contours, such as straight line segments and to fit the model outlines\u201d and section 4.1.3) and page 39 (edge matching) Gu Paragraphs 52, 62, and 71]; and wherein said controller to perform texture composition, if said contour and said edge feature are aligned [Wang Sections 1 \u2013 3 as well as Lepetit Section 2.3.3 Page 17 (alignment), 4.1 (\u201cextract image contours, such as straight line segments and to fit the model outlines\u201d and section 4.1.3) as well as Gu Paragraph 71 (RANSAC technique taught and rendered obvious to one of ordinary skill in the art) and 107 (alignment of contours / edges)], by calculating edge coordinates for each vertex of a triangle on a three dimensional surface [Lepetit Section 4.3.4 (triangles generated for determining textures to correct via warping) as well as Gu Paragraphs 99 \u2013 104 (triangles generated based on matched pixels from edge detection algorithms and also for mesh generated using matched pixels from edge detection / alignment / matching), and 110 (triangles created based on edges shared)], applying geometric correction to remove imprecise image registration [Wang Section 3 as well as Lepetit Sections 2.1 (camera / image parameters K, R, and T), 2.3 including 2.3.5 (correcting geometric errors & camera pose / registration issues), and 2.5 (estimation of pose for registration including use of RANSAC algorithm) and further Lepetit Sections 4.2.2 (considering errors in matching including noisy gradients) and 4.4.4. (\u201cimproves the accuracy of the recovered pose\u201d) as well as Gu Paragraph 70 (noise / shadow corrections), 108 (refining surface models / image registration), and 111 (rotation matrices are considered a geometric correction and the \u201cglobal coordinate system\u201d is the image registration being corrected).], and binding a texel grid to an image patch that covers a valid projection to produce a valid image patch and updating a texture to generate said three dimensional model contour [Nehab Paragraphs 10 (where the \u201ctexel program\u201d taught is a grid to overlap to a region of interest which is regarded as the image patch.) and 65 \u2013 72 (using the texel program / grid for overlapping / rendering images with texture information in a region of interest) and 82 \u2013 86 (using the texels for regions of interest see Section 3.2) where Paragraph 70 teaches (\u201c3-D model\u201d) thus the technique is applicable for 3D processing.  The Examiner notes Lepetit's teaching of warping texture maps (Section 4.3.4) as a nexus to incorporate Nehab's texel teaching.  Wang Sections 1 and 3 as well as Lepetit Sections 4.3 (in particular Section 4.3.4 warping texture maps onto models), 4.4 (texturing patches in images in relation to matching and 4.4.1 and 4.4.2 with boundaries / regions / windows considered (e.g. 7x7 correlation windows) as well as Gu Paragraphs 66, 78 (window for an image patch and a matching score to determine validity \u2013 see paragraph 81 as well), 81, 82 (validity test), 104 (smoothing surfaces), 109 (a \u201cregion of interest\u201d can be considered as the claimed \u201cpatch\u201d in the broadest reasonable interpretation of the claim)].\n\nThe motivation to combine Wang with Kishikawa is to combine concepts in a related field of invention as the Applicant in \"virtual space and real world\" technologies [Wang Section I Paragraph 1].  The motivation to combine Lepetit with Wang and Kishikawa is to combine image processing concepts in the same field of invention of \u201cAugmented Reality\u201d [Lepetit Abstract Pages 1 \u2013 2].  The motivation to combine Gu with Lepetit, Wang and Kishikawa is to combine concepts in the related field of invention of \u2018image processing\u201d for \u201cthree dimensional digitization\u201d [Gu Paragraphs 2 and 3].  The motivation to combine Nehab with Gu, Lepetit, Wang, and Kishikawa is to combine concepts in the related field of invention of \u201cvector graphics descriptions to overlapping grid cells\u201d [Nehab Paragraph 2].\n\n\n\nClaim Rejections - 35 USC \u00a7 103\n\nThe following is a quotation of pre-AIA 35 U.S.C. 103(a) which forms the basis for all obviousness rejections set forth in this Office action:\n\n(a) A patent may not be obtained though the invention is not identically disclosed or described as set forth in section 102 of this title, if the differences between the subject matter sought to be patented and the prior art are such that the subject matter as a whole would have been obvious at the time the invention was made to a person having ordinary skill in the art to which said subject matter pertains.  Patentability shall not be negatived by the manner in which the invention was made.\n\n\n\n\n\nClaims 13 and 17 \u2013 20 are rejected under pre-AIA 35 U.S.C. 103(a) as being unpatentable over Kishikawa, and further in view of Scott, et al. (US PG PUB 2010/0045701 A1 referred to as \u201cScott\u201d throughout), Lepetit, et al. (Lepetit, V., Fua, P.; \u201cMonocular Model-Based 3D Tracking of Rigid Objects: A Survey\u201d Foundations and Trends in Computer Graphics and Vision.  Vol. 1, No. 1, 2005, pp 1 \u2013 89 referred to as \u201cLepetit\u201d throughout) [Cited in Previous Actions], Gu (US PG PUB 2006/0210146 A1 referred to as \"Gu\" throughout), and Nehab, et al. (US PG PUB 2008/0309676 A1 referred to as \u201cNehab\u201d throughout).\n\nRegarding claim 13, Kishikawa teaches a device to aid in creation of a virtual world and Scott explicitly teaches many sensors to integrate with the camera and fusion algorithms.  Lepetit teaches a variety of 3D image processing techniques including camera considerations, use of image patches, and edge matching.  Gu teaches further 3D image processing considerations including triangles / meshes, edge alignment, and corrections.  Nehab teaches usage of a texel matrix / grid for rendering 3D images.\n\nIt would have been obvious to one of ordinary skill in the art at the time the invention was made to combine Kishikawa's camera and imaging device to generate a virtual world with Scott\u2019s sensors and fusion techniques along with Lepetit\u2019s, Nehab\u2019s and Gu\u2019s image processing techniques.  The combination teaches a hardware controller [Kishikawa Figure 5 reference character 100 (system) which the Examiner notes performs control functions.  The Examiner further considers a computer as a controller (hardware controller) and thus Figure 9 reference character 27 (mobile computer) is also a controller (Kishikawa Paragraphs 91, 92, and 95).  Further Scott Figures 1 and 2 have elements of a control algorithm that could be considered as a control (e.g. fusion, rendering, estimating position, etc.).]; a camera coupled to said controller [Kishikawa Figure 9 reference character 20 (camera coupled to controller \u2013 Figure 9 reference character 27 \u2013 see remarks directed towards the claimed \u201ccontroller\u201d above.  Scott Paragraphs 56 and 59 teach the camera coupled to algorithms to compute position which is considered as \u201ccontrol\u201d to the Examiner.  Scott Figures 1 and 2 (see above)]; an inertial navigation system sensor coupled to said controller [Kishikawa Paragraphs 92 and 93 suggest an INS (GPS unit synchronized with the camera) and Paragraph 17 (description of functions in which one of ordinary skill would select an INS (inertial navigation system) to perform the functions.  Scott in Table I (The Examiner considers the DMLO (directly measured location and orientation) as suggesting and teaching the use of an INS since the relative position suffices the claimed limitation when considering Kishikawa\u2019s teaching as well) and Paragraphs 43, 45 (the combination of functions listed suggest an INS is present), 50 \u2013 56 (various sensors to use and functions of sensors), 59 (DMLO which performs the function of the INS), 81, and 82 (camera position is known) as well as Lepetit Page 78 (contemplates the use of an INS)]; wherein said controller to compare an input three dimensional geometric model contour with an edge feature of images from said camera [Lepetit Section 4.1 (\u201cextract image contours, such as straight line segments and to fit the model outlines\u201d and section 4.1.3) and page 39 (edge matching) Gu Paragraphs 52, 62, and 71]; and wherein said controller to perform texture composition, if said contour and said edge feature are aligned [Scott Paragraphs 73 \u2013 74 (edge alignment) as well as Lepetit Section 2.3.3 Page 17 (alignment), 4.1 (\u201cextract image contours, such as straight line segments and to fit the model outlines\u201d and section 4.1.3) as well as Gu Paragraph 71 (RANSAC technique taught and rendered obvious to one of ordinary skill in the art) and 107 (alignment of contours / edges)], by calculating edge coordinates for each vertex of a triangle on a three dimensional surface [Lepetit Section 4.3.4 (triangles generated for determining textures to correct via warping) as well as Gu Paragraphs 99 \u2013 104 (triangles generated based on matched pixels from edge detection algorithms and also for mesh generated using matched pixels from edge detection / alignment / matching), and 110 (triangles created based on edges shared)], applying geometric correction to remove imprecise image registration [Scott Paragraphs 76 \u2013 80 (aligning axis / orientations is considered registration correction) as well as Lepetit Sections 2.1 (camera / image parameters K, R, and T), 2.3 including 2.3.5 (correcting geometric errors & camera pose / registration issues), and 2.5 (estimation of pose for registration including use of RANSAC algorithm) and further Lepetit Sections 4.2.2 (considering errors in matching including noisy gradients) and 4.4.4. (\u201cimproves the accuracy of the recovered pose\u201d) as well as Gu Paragraph 70 (noise / shadow corrections), 108 (refining surface models / image registration), and 111 (rotation matrices are considered a geometric correction and the \u201cglobal coordinate system\u201d is the image registration being corrected.], and binding a texel grid to an image patch that covers a valid projection to produce a valid image patch and updating a texture to generate said three dimensional model contour [Nehab Paragraphs 10 (where the \u201ctexel program\u201d taught is a grid to overlap to a region of interest which is regarded as the image patch.) and 65 \u2013 72 (using the texel program / grid for overlapping / rendering images with texture information in a region of interest) and 82 \u2013 86 (using the texels for regions of interest see Section 3.2) where Paragraph 70 teaches (\u201c3-D model\u201d) thus the technique is applicable for 3D processing.  The Examiner notes Lepetit's teaching of warping texture maps (Section 4.3.4) as a nexus to incorporate Nehab's texel teaching.  Lepetit Sections 4.3 (in particular Section 4.3.4 warping texture maps onto models), 4.4 (texturing patches in images in relation to matching and 4.4.1 and 4.4.2 with boundaries / regions / windows considered (e.g. 7x7 correlation windows) as well as Gu Paragraphs 66, 78 (window for an image patch and a matching score to determine validity \u2013 see paragraph 81 as well), 81, 82 (validity test), 104 (smoothing surfaces), 109 (a \u201cregion of interest\u201d can be considered as the claimed \u201cpatch\u201d in the broadest reasonable interpretation of the claim) and Scott Paragraphs 109 \u2013 133 (point localization / updating / adjusting algorithms given)].\n\nThe motivation to combine Scott with Kishikawa is to combine concepts in the same field of invention as the Applicant in \u201caugmented reality\u201d [Scott Paragraphs 3 and 4].  The motivation to combine Lepetit with Scott and Kishikawa is to combine image processing concepts in the same field of invention of \u201cAugmented Reality\u201d [Lepetit Abstract Pages 1 \u2013 2].  The motivation to combine Gu with Lepetit, Scott and Kishikawa is to combine concepts in the related field of invention of \u2018image processing\u201d for \u201cthree dimensional digitization\u201d [Gu Paragraphs 2 and 3].  The motivation to combine Nehab with Gu, Lepetit, Scott, and Kishikawa is to combine concepts in the related field of invention of \u201cvector graphics descriptions to overlapping grid cells\u201d [Nehab Paragraph 2].\n\nRegarding claim 17, Kishikawa teaches a device to aid in creation of a virtual world and Scott explicitly teaches many sensors to integrate with the camera and fusion algorithms.\n\nIt would have been obvious to one of ordinary skill in the art at the time the invention was made to combine Kishikawa's camera and imaging device to generate a virtual world with Scott\u2019s sensors and fusion techniques.  The combination teaches hardware controller combines inertial navigation system sensor data [See claim 13 regarding discussion of INS data and claimed \u201ccontroller\u201d] and camera images [Kishikawa Paragraph 131 (the photograph)] for texture mapping [Scott Paragraphs 48 and 49 (texture mapping with camera image and pose / position (using INS / DMLO data to determine the position information for the camera)) and Kishikawa Paragraphs 127 \u2013 132 Figure 5 reference character 140 (Texture Creation module) as well as Paragraphs 73 \u2013 75 (combining images and position in relation to adding texture to a building / feature in the virtual map) and Paragraphs 87 \u2013 89.].  The motivation to combine Scott with Kishikawa is to combine concepts in the same field of invention as the Applicant in \u201caugmented reality\u201d [Scott Paragraphs 3 and 4].\n\nRegarding claim 18, Kishikawa teaches a device to aid in creation of a virtual world and Scott explicitly teaches many sensors to integrate with the camera and fusion algorithms.\n\nIt would have been obvious to one of ordinary skill in the art at the time the invention was made to combine Kishikawa's camera and imaging device to generate a virtual world with Scott\u2019s sensors and fusion techniques.  The combination teaches sensor fusion to fuse relative orientation parameters based on camera image sequences [Kishikawa Paragraphs 75, 95 and 95 (position with each photograph taken for fusion / incorporation) and Scott Figure 1 reference character 117 (fusion algorithm) and Scott Paragraphs 49 \u2013 50 (sensor fusion with sensors used between camera views / pictures taken), 56 \u2013 57, 81 \u2013 82 (fusion with camera imagery), and Paragraphs 134 \u2013 141 (fusion techniques)] with inertial navigation system sensor inputs [See claim 13 regarding INS information \u2013 Scott Table I highlights possible inputs to be used in a fusion algorithm].  The motivation to combine Scott with Kishikawa is to combine concepts in the same field of invention as the Applicant in \u201caugmented reality\u201d [Scott Paragraphs 3 and 4].\n\nRegarding claim 19, Kishikawa teaches a device to aid in creation of a virtual world and Scott explicitly teaches many sensors to integrate with the camera and fusion algorithms.\n\nIt would have been obvious to one of ordinary skill in the art at the time the invention was made to combine Kishikawa's camera and imaging device to generate a virtual world with Scott\u2019s sensors and fusion techniques.  The combination teaches a global positioning system receiver [Scott Figure 2 reference character 112 (GPS) and Kishikawa Figure 9 reference character 25 (GPS)].  The motivation to combine Scott with Kishikawa is to combine concepts in the same field of invention as the Applicant in \u201caugmented reality\u201d [Scott Paragraphs 3 and 4].\n\nRegarding claim 20, Kishikawa teaches a device to aid in creation of a virtual world and Scott explicitly teaches many sensors to integrate with the camera and fusion algorithms.\n\nIt would have been obvious to one of ordinary skill in the art at the time the invention was made to combine Kishikawa's camera and imaging device to generate a virtual world with Scott\u2019s sensors and fusion techniques.  The combination teaches an accelerometer [Scott Table I (3-axis accelerometer) and Paragraph 43 and Figure 2 reference character 115].  The motivation to combine Scott with Kishikawa is to combine concepts in the same field of invention as the Applicant in \u201caugmented reality\u201d [Scott Paragraphs 3 and 4].\n\n\n\nClaims 14 \u2013 15 are rejected under pre-AIA 35 U.S.C. 103(a) as being unpatentable over Kishikawa, Scott, Lepetit, Gu, Nehab, and further in view of Nasiri, et al. (US PG PUB 2009/0303204 A1 referred to as \u201cNasiri\u201d throughout).\n\nRegarding claim 14, Kishikawa teaches a device to aid in creation of a virtual world and Scott explicitly teaches many sensors to integrate with the camera and fusion algorithms.  Lepetit teaches a variety of 3D image processing techniques including camera considerations, use of image patches, and edge matching.  Gu teaches further 3D image processing considerations including triangles / meshes, edge alignment, and corrections.  Nehab teaches usage of a texel matrix / grid for rendering 3D images.  Nasiri teaches explicit embodiments / devices.\n\nIt would have been obvious to one of ordinary skill in the art at the time the invention was made to combine Kishikawa's camera and imaging device to generate a virtual world with Scott\u2019s sensors and fusion techniques along with Lepetit\u2019s, Nehab\u2019s, and Gu\u2019s image processing techniques in devices taught by Nasiri.  The combination teaches a mobile Internet device [Kishikawa Paragraph 91 (mobile computer) and Nasiri Paragraph 50].  The motivation to combine Scott with Kishikawa is to combine concepts in the same field of invention as the Applicant in \u201caugmented reality\u201d [Scott Paragraphs 3 and 4].  The motivation to combine Lepetit with Scott and Kishikawa is to combine image processing concepts in the same field of invention of \u201cAugmented Reality\u201d [Lepetit Abstract Pages 1 \u2013 2].  The motivation to combine Gu with Lepetit, Scott and Kishikawa is to combine concepts in the related field of invention of \u2018image processing\u201d for \u201cthree dimensional digitization\u201d [Gu Paragraphs 2 and 3].  The motivation to combine Nehab with Gu, Lepetit, Scott, and Kishikawa is to combine concepts in the related field of invention of \u201cvector graphics descriptions to overlapping grid cells\u201d [Nehab Paragraph 2].  The motivation to combine Nasiri with Nehab, Gu, Lepetit, Scott and Kishikawa is to combine concepts in the related field of invention as the Applicant in developing devices to use with \u201cvirtual words\u201d [Nasiri Paragraphs 153 \u2013 159].\n\nRegarding claim 15, Kishikawa teaches a device to aid in creation of a virtual world and Scott explicitly teaches many sensors to integrate with the camera and fusion algorithms.  Lepetit teaches a variety of 3D image processing techniques including camera considerations, use of image patches, and edge matching.  Gu teaches further 3D image processing considerations including triangles / meshes, edge alignment, and corrections.  Nehab teaches usage of a texel matrix / grid for rendering 3D images.  Nasiri teaches explicit embodiments / devices.\n\nIt would have been obvious to one of ordinary skill in the art at the time the invention was made to combine Kishikawa's camera and imaging device to generate a virtual world with Scott\u2019s sensors and fusion techniques along with Lepetit\u2019s, Nehab\u2019s, and Gu\u2019s image processing techniques in devices taught by Nasiri.  The combination teaches a mobile wireless device [Kishikawa Paragraph 91 (mobile computer) and Nasiri Paragraph 50].  The motivation to combine Scott with Kishikawa is to combine concepts in the same field of invention as the Applicant in \u201caugmented reality\u201d [Scott Paragraphs 3 and 4].  The motivation to combine Lepetit with Scott and Kishikawa is to combine image processing concepts in the same field of invention of \u201cAugmented Reality\u201d [Lepetit Abstract Pages 1 \u2013 2].  The motivation to combine Gu with Lepetit, Scott and Kishikawa is to combine concepts in the related field of invention of \u2018image processing\u201d for \u201cthree dimensional digitization\u201d [Gu Paragraphs 2 and 3].  The motivation to combine Nehab with Gu, Lepetit, Scott, and Kishikawa is to combine concepts in the related field of invention of \u201cvector graphics descriptions to overlapping grid cells\u201d [Nehab Paragraph 2].  The motivation to combine Nasiri with Nehab, Gu, Lepetit, Scott and Kishikawa is to combine concepts in the related field of invention as the Applicant in developing devices to use with \u201cvirtual words\u201d [Nasiri Paragraphs 153 \u2013 159].\n\n\n\nClaims 16 is rejected under pre-AIA 35 U.S.C. 103(a) as being unpatentable over Kishikawa, Scott, Lepetit, Gu, Nehab, and further in view of Adoni, et al. (US PG PUB 2011/0107239 A1 referred to as \u201cAdoni\u201d throughout).\n\nRegarding claim 16, Kishikawa teaches a device to aid in creation of a virtual world and Scott explicitly teaches many sensors to integrate with the camera and fusion algorithms.  Lepetit teaches a variety of 3D image processing techniques including camera considerations, use of image patches, and edge matching.  Gu teaches further 3D image processing considerations including triangles / meshes, edge alignment, and corrections.  Nehab teaches usage of a texel matrix / grid for rendering 3D images.  Adoni explicitly teaches the use of \u201cmirror worlds\u201d.\n\nIt would have been obvious to one of ordinary skill in the art at the time the invention was made to combine Kishikawa's camera and imaging device to generate a virtual world with Scott\u2019s sensors and fusion techniques along with Lepetit\u2019s, Nehab\u2019s, and Gu\u2019s image processing techniques with Adoni\u2019s use of a virtual / mirror world.  The combination teaches creates a Mirror World [Scott Paragraph 142 (augmented reality) and Adoni Paragraph 130].  The Examiner regards the usage of phrases such as \u201caugmented reality\u201d, \u201cvirtual world\u201d, and \u201cmirror world\u201d as synonymous and thus obvious to one of ordinary skill in the art.\n\nThe motivation to combine Scott with Kishikawa is to combine concepts in the same field of invention as the Applicant in \u201caugmented reality\u201d [Scott Paragraphs 3 and 4].  The motivation to combine Lepetit with Scott and Kishikawa is to combine image processing concepts in the same field of invention of \u201cAugmented Reality\u201d [Lepetit Abstract Pages 1 \u2013 2].  The motivation to combine Gu with Lepetit, Scott and Kishikawa is to combine concepts in the related field of invention of \u2018image processing\u201d for \u201cthree dimensional digitization\u201d [Gu Paragraphs 2 and 3].  The motivation to combine Nehab with Gu, Lepetit, Scott, and Kishikawa is to combine concepts in the related field of invention of \u201cvector graphics descriptions to overlapping grid cells\u201d [Nehab Paragraph 2].  The motivation to combine Adoni with Nehab, Gu, Lepetit, Scott and Kishikawa is to combine concepts in developed augmented reality / virtual worlds / mirror worlds which are the same field of invention as the Applicant [Adoni Paragraph 5].\n\n\n\nConclusion\n\nThe prior art made of record and not relied upon is considered pertinent to applicant's disclosure.  The Examiner attached search notes utilizing NPL (Non-Patent Literature) which the Applicant is strongly urged to consider as being pertinent to the claims prosecuted.  Further, Holmes (US PG PUB 2006/0056732 A1 referred to as \u201cHolmes\u201d throughout) teaches drawing triangles based on edge matching and filling holes.  Kuranov, et al. (US PG PUB 2008/0253685 A1 referred to as \"Kuranov\" throughout) teaching image processing and depth considerations when stitching together images to generate a panoramic image including drawing triangles as a function of matched edges.  Ichimura, et al. (US PG PUB 2010/0156896 A1 referred to as \u201cIchimura\u201d throughout) teaches triangles drawn for alignment of objects in robotic systems.\n\nApplicant's amendment necessitated the new ground(s) of rejection presented in this Office action.  Accordingly, THIS ACTION IS MADE FINAL.  See MPEP \u00a7 706.07(a).  Applicant is reminded of the extension of time policy as set forth in 37 CFR 1.136(a).  \n\nA shortened statutory period for reply to this final action is set to expire THREE MONTHS from the mailing date of this action.  In the event a first reply is filed within TWO MONTHS of the mailing date of this final action and the advisory action is not mailed until after the end of the THREE-MONTH shortened statutory period, then the shortened statutory period will expire on the date the advisory action is mailed, and any extension fee pursuant to 37 CFR 1.136(a) will be calculated from the mailing date of the advisory action.  In no event, however, will the statutory period for reply expire later than SIX MONTHS from the date of this final action. \n\nAny inquiry concerning this communication or earlier communications from the examiner should be directed to Tyler W. Sullivan whose telephone number is (571)270-5684.  The examiner can normally be reached on IFP.\n\nIf attempts to reach the examiner by telephone are unsuccessful, the examiner\u2019s supervisor, David Czekaj can be reached on (571) 272-7327.  The fax phone number for the organization where this application or proceeding is assigned is 571-273-8300.\n\nInformation regarding the status of an application may be obtained from the Patent Application Information Retrieval (PAIR) system.  Status information for published applications may be obtained from either Private PAIR or Public PAIR.  Status information for unpublished applications is available through Private PAIR only.  For more information about the PAIR system, see http://pair-direct.uspto.gov. Should you have questions on access to the Private PAIR system, contact the Electronic Business Center (EBC) at 866-217-9197 (toll-free). If you would like assistance from a USPTO Customer Service Representative or access to the automated information system, call 800-786-9199 (IN USA OR CANADA) or 571-272-1000.\n\n\n\n\n\n\n\n\n\n\n\n/T. W. S./\n\nExaminer, Art Unit 2487\n\n\n\n/Dave Czekaj/\n\nSupervisory Patent Examiner, Art Unit 2487\n\n\n\n\n    \n", "appl_id": 13000099, "file_dt": 1310097600.0, "effective_filing_dt": 978325200.0, "inv_subj_matter_ty": "UTL", "appl_ty": "REGULAR", "dn_examiner_no": "88343 ", "dn_dw_dn_gau_cd": "2487  ", "dn_pto_art_class_no": "348", "dn_pto_art_subclass_no": "113000", "confirm_no": 1536, "dn_intppty_cust_no": 47795.0, "atty_dkt_no": "ITL.2271US (P33273)", "dn_nsrd_curr_loc_cd": "e    ", "dn_nsrd_curr_loc_dt": 978325200.0, "app_status_no": 161, "app_status_dt": 1434254400.0, "wipo_pub_no": NaN, "patent_no": "          ", "patent_issue_dt": 978325200.0, "abandon_dt": 1420606800.0, "disposal_type": "ABANDONED", "se_in": "N", "pct_no": "                 ", "invn_ttl_tx": "Extracting and Mapping Three Dimensional Features from Geo-Referenced Images", "aia_in": "N", "continuity_type": "   ", "frgn_priority_clm": "N", "usc_119_met": "N", "fig_qt": 0, "indp_claim_qt": 3, "efctv_claims_qt": 20, "doc_date": 1412568000.0}